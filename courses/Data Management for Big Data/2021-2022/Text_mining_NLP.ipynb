{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_mining_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-jKacTzBqa_U"
      },
      "outputs": [],
      "source": [
        "# In this tutorial, we will see how it is possible to extract meaningful information from text data\n",
        "# To such an extent, we are going to exploit the Python package \"spaCy\": https://spacy.io/\n",
        "\n",
        "# We are first going to provide a brief introduction to natural language processing (NLP) with spaCy\n",
        "# This includes some basic operations for cleaning and analyzing text data \n",
        "# Finally, we will practically deal with text classification, using some real-world data \n",
        "\n",
        "# Tutorial adapted from: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCy is an open-source natural language processing library for Python\n",
        "# It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently\n",
        "# Thus, it is an \"industry-level\" framework for NLP\n",
        "\n",
        "# First of all, let us install such a useful library"
      ],
      "metadata": {
        "id": "838WLzHFt6R3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall spacy --yes\n",
        "!pip install spacy==2.1.0"
      ],
      "metadata": {
        "id": "wtGH2dyvuORA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall neuralcoref --yes\n",
        "!pip install neuralcoref"
      ],
      "metadata": {
        "id": "Yb4AUBu5IlsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us start our text analytics task with tokenization, which involves breaking the text into pieces, called tokens. For now, let us just ignore n-grams.\n",
        "\n",
        "\n",
        "# Define an exemplary text that we are going to process\n",
        "text = \"One morning I shot an elephant in my pajamas. How he got into my pajamas I'll never know  .\"\n",
        "\n",
        "import spacy \n",
        "\n",
        "# Load English text processing pipeline\n",
        "# In spaCy, all tasks are handled by means of pipelines (models)\n",
        "# A pipeline takes an input text and applies, in sequence, several functions to it, typically intended (and trained according to different methodologies) for a specific language\n",
        "# It is possible for a user to define a custom pipeline, or to rely on predefined ones, as we do here with \"en\"\n",
        "# In any case, also default pipelines can be customized\n",
        "\n",
        "# The following instruction downloads one of such default models\n",
        "\n",
        "!python -m spacy download en\n"
      ],
      "metadata": {
        "id": "J_s62ByIJUo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the model\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "print(\"This are the components of the nlp model:\", nlp.pipe_names)\n",
        "\n",
        "# As we shall see, \"nlp\" converts the text into a list of token objects, each with a set of properties\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Let us retrieve the list of generated tokens from my_doc\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "  token_list.append(token.text) # note that the token is not a string, but a specific object used within spaCy\n",
        "print(token_list)\n",
        "\n",
        "# The tokens however still contain whitespace punctuation characters..."
      ],
      "metadata": {
        "id": "UZqG7gvbv234"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us retrieve again the list of generated tokens from my_doc, but this time excluding punctuation and whitespace characters\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "  if not(token.is_punct or token.is_space):\n",
        "    token_list.append(token.text)\n",
        "print(token_list)"
      ],
      "metadata": {
        "id": "v5qZRmoRycPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we want to remove stopwords, i.e., those very frequent words that are not very useful for analysis purposes\n",
        "# spaCy already includes a list of stopwords that we can inspect\n",
        "\n",
        "#importing stop words from English language.\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "#Printing the total number of stop words:\n",
        "print('Number of stop words: %d' % len(STOP_WORDS))\n",
        "\n",
        "#Printing first ten stop words:\n",
        "print('First ten stop words: %s' % list(STOP_WORDS)[:20])"
      ],
      "metadata": {
        "id": "Oc58T3sIz3be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let us include the step that filters the stopwords from our example text\n",
        "\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "  if not(token.is_punct or token.is_space or token.is_stop):\n",
        "    token_list.append(token)\n",
        "print(token_list)"
      ],
      "metadata": {
        "id": "ImttNGhXt_s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we come to the text normalization part\n",
        "# Here, we want to transform each word in its root form (lemma)\n",
        "# This can be done by the stemming process, which was already carried out by the \"nlp\" function\n",
        "\n",
        "# Let us consider tokens in their root form\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "  if not(token.is_punct or token.is_space or token.is_stop):\n",
        "    token_list.append(token.lemma_)\n",
        "print(token_list)"
      ],
      "metadata": {
        "id": "NQZL1yQ3u0ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging\n",
        "# Part-Of-Speech (POS) tagging involves associating to each token its grammar role within the sentence\n",
        "# This may be useful to perform some kinds of text analytics tasks\n",
        "# Again, also POS has been already carried out by our very useful \"nlp\" function\n",
        "\n",
        "\n",
        "# Let us associate POS information to each token\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "  if not(token.is_punct or token.is_space or token.is_stop):\n",
        "    token_list.append((token, token.lemma_, token.pos_))\n",
        "print(token_list)"
      ],
      "metadata": {
        "id": "vd0KP_wwu2H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another cool thing regarding spaCy is that it also performs NER\n",
        "# NER stands for Named Entity Recognition\n",
        "# NER is a more advanced form of language processing that identifies important elements in the text\n",
        "# like places, people, organizations, and so on\n",
        "# This is really helpful for quickly extracting information from text, since you can quickly pick out \n",
        "# important topics or indentify key sections of text\n",
        "\n",
        "# We will change our example text now\n",
        "\n",
        "nytimes = nlp(\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases. At least 285 people have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday. The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\")\n",
        "\n",
        "entities=[(i, i.label_, i.label) for i in nytimes.ents]\n",
        "display(entities)"
      ],
      "metadata": {
        "id": "i7Xd7_mX1scp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For visualization purposes we can also importing the next package\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(nytimes, style = \"ent\", jupyter = True)"
      ],
      "metadata": {
        "id": "4MZfTrj417qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependency Parsing\n",
        "# Depenency parsing is a language processing technique that allows us to better determine \n",
        "# the meaning of a sentence by analyzing how it’s constructed\n",
        "# Specifically, we want to determine how the individual words relate to each other.\n",
        "\n",
        "# Let us change the exemplary text again\n",
        "analyzed_text = nlp(\"I must confess, I was born at a very early age.\")\n",
        "\n",
        "displacy.render(analyzed_text, style=\"dep\", jupyter= True)\n"
      ],
      "metadata": {
        "id": "pS2LWpDD2MTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes we have that the same subject or object is referred to as in the text multiple times in different ways\n",
        "# This can be the case, for instance, of pronouns\n",
        "# The task by which we recognize all occurrences of a same entity in the text is called Coreference Resolution\n",
        "\n",
        "# Such a task is not carried out by the \"nlp\" function by default, but we can add it to the pipeline\n",
        "# Add neural coref to SpaCy's pipe\n",
        "import spacy\n",
        "import neuralcoref\n",
        "nlp = spacy.load('en')\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "# Another text...\n",
        "doc = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
        "for ent in doc.ents:\n",
        "    print(ent._.coref_cluster)"
      ],
      "metadata": {
        "id": "SId48so23kMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCy has also the support for word vectors, or word embeddings\n",
        "# The idea is that each word gets mapped into a latent, multidimensional space\n",
        "# The mapping from words to n-dimensional vectors somehow preserves semantic content of words\n",
        "# For instance, synonims should end up close in the latent space\n",
        "# The latent representation also allows us to capture many semantic connections within words\n",
        "\n",
        "# We need to download and employ another model for the English language to to that\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "nlp = spacy.load('en')"
      ],
      "metadata": {
        "id": "DQC3HLh9SdmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us get the embeddings of the word \"dog\"\n",
        "\n",
        "dog = nlp('dog')\n",
        "print(dog.vector.shape)\n",
        "print(dog.vector)"
      ],
      "metadata": {
        "id": "xoq9Bto0WFDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now define a similarity function between arrays\n",
        "def cosine_similarity(vec1, vec2):\n",
        "  from scipy import spatial\n",
        "  return 1 - spatial.distance.cosine(vec1,vec2)\n",
        "\n",
        "# We observe that the wolf vector is the closest one to the dog vector, followed by those of bear and elephant\n",
        "print(cosine_similarity(dog.vector, nlp('wolf').vector))\n",
        "print(cosine_similarity(dog.vector, nlp('bear').vector))\n",
        "print(cosine_similarity(dog.vector, nlp('elephant').vector))\n"
      ],
      "metadata": {
        "id": "TdWjFDWjUfSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have seen some stuff regarding spaCy, we are going to consider a text classification task\n",
        "\n",
        "# Let us now load our referece dataset, which is about reviews regarding Amazon's Alexa smart home speaker\n",
        "# The data is available at: https://github.com/dslab-uniud/teaching/blob/main/courses/Data%20Management%20for%20Big%20Data/2021-2022/amazon_alexa.tsv \n",
        "\n",
        "# Download the TSV file, then, upload it into the Colab\n",
        "# To do that, you can just drag the file onto the \"file\" area of the Colab"
      ],
      "metadata": {
        "id": "YFMHLiyzBAix"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let us read the content of the file\n",
        "# To such an extent, we are relying on Python's Pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Also, for our analysis tasks we are going to need some sklearn libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "dataset = pd.read_csv(\"./amazon_alexa.tsv\", sep=\"\\t\")\n",
        "dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "display(dataset)\n",
        "\n",
        "# Ok, our dataset is composed of 3150 reviews, each described by 5 columns: \n",
        "#  - rating denotes the rating each user gave the Alexa (out of 5)\n",
        "#  - date indicates the date of the review\n",
        "#  - variation describes which model the user reviewed\n",
        "#  - verified_reviews contains the text of each review\n",
        "#  - feedback contains a sentiment label, with 1 denoting positive sentiment (the user liked it) and 0 denoting negative sentiment (the user didn’t)\n"
      ],
      "metadata": {
        "id": "vUJTJFBoBCkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us prepare predictor and label data and divide them into training and test splits\n",
        "\n",
        "X = dataset['verified_reviews'].values\n",
        "y = dataset['feedback'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "0NTuOA3qr5EC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, for text pre-processing purposes, as usual, we are loading our spaCy model\n",
        "nlp = spacy.load('en', disable = ['ner'])\n",
        "\n",
        "# Creating our custom tokenizer function, inspired from what we have seen above\n",
        "def spacy_tokenizer(sentence):\n",
        "  token_list = []\n",
        "  for token in nlp(sentence):\n",
        "    if not(token.is_punct or token.is_space or token.is_stop) and token.is_alpha:\n",
        "      token_list.append(str(token.lemma_).lower())\n",
        "  return token_list\n"
      ],
      "metadata": {
        "id": "kJVasedWsKNy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer converts a given text into a matrix of word occurrences, giving to it a structured representation\n",
        "# It is also capable of handling n-grams instead of single words, by means of the parameter ngram_range=(lower_bound, upper_bound)\n",
        "# We also ignore words that appear in less than 1% of the reviews (min_df)\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1), min_df=int(len(dataset)*0.01))\n",
        "vectorizer.fit(X_train) # fitting the vectorizer on training set data only\n",
        "\n",
        "# Vectorizing both trainig and test data\n",
        "X_train_vect = vectorizer.transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "ioSHxvBGkpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The vectorizer builds a list of attributes\n",
        "# Each sentence is going to be represented by the number of times these words occur in it\n",
        "\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "tyapueqSmIIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also visualize the transformed dataset\n",
        "\n",
        "X_array = X_train_vect.toarray()\n",
        "display(pd.DataFrame(data=X_array, columns=vectorizer.get_feature_names()).iloc[30:35])\n",
        "\n",
        "print(X_train[31]) # notice how both added and adding have been counted as occurrences of add"
      ],
      "metadata": {
        "id": "pAMwhrfWqezR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the logistic model to predict the sentiment of a review\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_vect, y_train)\n",
        "\n",
        "preds = classifier.predict(X_test_vect)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(preds, y_test))"
      ],
      "metadata": {
        "id": "tYUQkCoHrTtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can try for example the following:\n",
        "#   - what happens if you consider n-grams instead of single words?\n",
        "#   - and if you replace CountVectorizer with TfidfVectorizer? https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "#   - can we try to predict the rating instead of the feedback? Maybe through a regression task?"
      ],
      "metadata": {
        "id": "ZALfMOoMxBl5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3CEefz_I0cpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}